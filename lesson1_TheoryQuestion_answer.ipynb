{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Can you come up out 3 sceneraies which use AI methods?\n",
    "Ans: Chat robots, stock prediction, money laundering, auto driving. \n",
    "\n",
    "1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "Ans: With git shell, we can synchronize the local repository and the remote repository, and share and work on the project with others.\n",
    "    Compared to origin IDE, these two IDE are more convenient and show more information about the codes.\n",
    "\n",
    "2. What's the Probability Model?\n",
    "Ans: Use the probability theory to determine which sentence have the biggest chance to be said in the real life.\n",
    "\n",
    "3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "Ans: Serving robots, chat robots, voice recognition.  \n",
    "\n",
    "4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "Ans: Using probability to generate the sentence more similar to the natural one.\n",
    "    Difficulties: the word and sentence themselves have polysemy and ambiguity, and there're many sentences in reality lacking some parts. \n",
    "\n",
    "5. What's the Language Model;\n",
    "Ans: language_model(String)=Probabilty(String)âˆˆ(0,1)\n",
    "    Pro(w1w2w3w4)=P(w1|w2w3w4)P(w2|w3w4)P(w3|w4)P(w4)\n",
    "\n",
    "6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "Ans: QA system, such as Siri.\n",
    "\n",
    "7. What's the 1-gram language model;\n",
    "Ans: Different from the answer in question5, the appearing possibility of every word is independent in the 1-gram language model. The formula is shown as below:\n",
    "    Pro(w1w2w3w4)=P(w1)P(w2)P(w3)P(w4)\n",
    "\n",
    "8. What's the disadvantages and advantages of 1-gram language model;\n",
    "Ans: Disadvantages: the model is not accurate.\n",
    "    Advantages: the model is easy and has relatively smaller amount of calculation\n",
    "\n",
    "9. What't the 2-gram models;\n",
    "Ans: Different from the answer in question5, every two adjacent words have dependent appearing possibility in the 2-gram language model. The formula is shown as below:\n",
    "    Pro(w1w2w3w4)=P(w1|w2)P(w2|w3)P(w3|w4)P(w4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
